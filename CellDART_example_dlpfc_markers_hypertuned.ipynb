{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellDART Example Code: mouse brain \n",
    "## (10x Visium of anterior mouse brain + scRNA-seq data of mouse brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31689/3903643673.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf  # TensorFlow registers PluggableDevices here.\n",
    "from tqdm.autonotebook import tqdm\n",
    "import yaml\n",
    "\n",
    "from CellDART import da_cellfraction\n",
    "from src.da_utils import data_loading\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # tf.config.set_logical_device_configuration(\n",
    "        #     gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=8192)]\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_SEEDS = (3679, 343, 25, 234, 98098)\n",
    "MODEL_SEEDS = (2353, 24385, 284, 86322, 98237)\n",
    "MODEL_DIR = \"model_FINAL\"\n",
    "SEED_OVERRIDE = None\n",
    "\n",
    "CONFIGS_DIR = \"configs\"\n",
    "CONFIG_FNAME = \"celldart-final-dlpfc-ht.yml\"\n",
    "\n",
    "# BOOTSTRAP = False\n",
    "# BOOTSTRAP_ROUNDS = 10\n",
    "# BOOTSTRAP_ALPHAS = [0.6, 1 / 0.6]\n",
    "\n",
    "MODEL_NAME = \"CellDART_original\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_params:\n",
      "  all_genes: false\n",
      "  data_dir: ../AGrEDA/data\n",
      "  dset: dlpfc\n",
      "  n_markers: 40\n",
      "  n_mix: 3\n",
      "  n_spots: 100000\n",
      "  samp_split: true\n",
      "  sc_id: GSE144136\n",
      "  scaler_name: minmax\n",
      "  st_id: spatialLIBD\n",
      "  st_split: false\n",
      "lib_params:\n",
      "  manual_seed: 1846326316\n",
      "model_params:\n",
      "  celldart_kwargs:\n",
      "    bn_momentum: 0.01\n",
      "    emb_dim: 32\n",
      "  model_version: gen_dlpfc_dlpfc-9356\n",
      "train_params:\n",
      "  alpha: 2.0\n",
      "  alpha_lr: 10\n",
      "  batch_size: 512\n",
      "  initial_train_epochs: 10\n",
      "  lr: 0.0001\n",
      "  n_iter: 15000\n",
      "  pretraining: true\n",
      "  reverse_val: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(CONFIGS_DIR, MODEL_NAME, CONFIG_FNAME), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "lib_params = config[\"lib_params\"]\n",
    "data_params = config[\"data_params\"]\n",
    "model_params = config[\"model_params\"]\n",
    "train_params = config[\"train_params\"]\n",
    "\n",
    "rewrite_config = False\n",
    "if not \"pretraining\" in train_params:\n",
    "    train_params[\"pretraining\"] = True\n",
    "    rewrite_config = True\n",
    "if not \"lr\" in train_params:\n",
    "    train_params[\"lr\"] = 0.001\n",
    "    rewrite_config = True\n",
    "\n",
    "if rewrite_config:\n",
    "    with open(os.path.join(CONFIGS_DIR, MODEL_NAME, CONFIG_FNAME), \"w\") as f:\n",
    "        yaml.safe_dump(config, f)\n",
    "\n",
    "tqdm.write(yaml.safe_dump(config))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data load\n",
    "### load scanpy data - 10x datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS seed: 3679, model seed: 2353\n",
      "Adversarial training for slides dict_keys(['151507', '151508', '151509', '151510', '151669', '151670', '151671', '151672', '151674', '151676']): \n",
      "Train on 100000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 2s 18us/sample - loss: 1.0519 - mae: 0.0310\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 1s 10us/sample - loss: 0.6670 - mae: 0.0228\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.5809 - mae: 0.0208\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 1s 10us/sample - loss: 0.5333 - mae: 0.0197\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 1s 10us/sample - loss: 0.4957 - mae: 0.0188\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 1s 10us/sample - loss: 0.4593 - mae: 0.0179\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.4241 - mae: 0.0170\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3911 - mae: 0.0162\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3597 - mae: 0.0153\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3319 - mae: 0.0146\n",
      "initial_train_done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38906321561336515\n",
      "0.38906321561336515\n",
      "Iteration 99, source loss =  3.235, discriminator acc = 0.990\n",
      "Iteration 199, source loss =  3.526, discriminator acc = 0.759\n",
      "Iteration 299, source loss =  2.094, discriminator acc = 0.789\n",
      "Iteration 399, source loss =  1.855, discriminator acc = 0.885\n",
      "Iteration 499, source loss =  1.622, discriminator acc = 0.911\n",
      "Iteration 599, source loss =  1.307, discriminator acc = 0.888\n",
      "Iteration 699, source loss =  1.274, discriminator acc = 0.649\n",
      "Iteration 799, source loss =  1.203, discriminator acc = 0.923\n",
      "Iteration 899, source loss =  1.352, discriminator acc = 0.696\n",
      "Iteration 999, source loss =  1.354, discriminator acc = 0.770\n",
      "Iteration 1099, source loss =  2.369, discriminator acc = 0.525\n",
      "Iteration 1199, source loss =  1.651, discriminator acc = 0.743\n",
      "Iteration 1299, source loss =  2.315, discriminator acc = 0.899\n",
      "Iteration 1399, source loss =  1.860, discriminator acc = 0.966\n",
      "Iteration 1499, source loss =  1.828, discriminator acc = 0.964\n",
      "Iteration 1599, source loss =  1.772, discriminator acc = 0.940\n",
      "Iteration 1699, source loss =  2.084, discriminator acc = 0.930\n",
      "Iteration 1799, source loss =  1.672, discriminator acc = 0.907\n",
      "Iteration 1899, source loss =  1.607, discriminator acc = 0.211\n",
      "Iteration 1999, source loss =  1.308, discriminator acc = 0.977\n",
      "Iteration 2099, source loss =  1.478, discriminator acc = 0.984\n",
      "Iteration 2199, source loss =  1.409, discriminator acc = 0.936\n",
      "Iteration 2299, source loss =  1.373, discriminator acc = 0.972\n",
      "Iteration 2399, source loss =  1.441, discriminator acc = 0.925\n",
      "Iteration 2499, source loss =  1.302, discriminator acc = 0.779\n",
      "Iteration 2599, source loss =  1.276, discriminator acc = 0.816\n",
      "Iteration 2699, source loss =  1.314, discriminator acc = 0.815\n",
      "Iteration 2799, source loss =  1.234, discriminator acc = 0.812\n",
      "Iteration 2899, source loss =  1.251, discriminator acc = 0.550\n",
      "Iteration 2999, source loss =  1.217, discriminator acc = 0.825\n",
      "Iteration 3099, source loss =  1.206, discriminator acc = 0.663\n",
      "Iteration 3199, source loss =  1.220, discriminator acc = 0.848\n",
      "Iteration 3299, source loss =  1.146, discriminator acc = 0.738\n",
      "Iteration 3399, source loss =  1.203, discriminator acc = 0.873\n",
      "Iteration 3499, source loss =  1.241, discriminator acc = 0.433\n",
      "Iteration 3599, source loss =  1.268, discriminator acc = 0.680\n",
      "Iteration 3699, source loss =  1.251, discriminator acc = 0.736\n",
      "Iteration 3799, source loss =  1.385, discriminator acc = 0.378\n",
      "Iteration 3899, source loss =  1.245, discriminator acc = 0.972\n",
      "Iteration 3999, source loss =  1.230, discriminator acc = 0.981\n",
      "Iteration 4099, source loss =  1.394, discriminator acc = 0.565\n",
      "Iteration 4199, source loss =  1.333, discriminator acc = 0.548\n",
      "Iteration 4299, source loss =  1.220, discriminator acc = 0.699\n",
      "Iteration 4399, source loss =  1.242, discriminator acc = 0.705\n",
      "Iteration 4499, source loss =  1.146, discriminator acc = 0.798\n",
      "Iteration 4599, source loss =  1.150, discriminator acc = 0.887\n",
      "Iteration 4699, source loss =  1.170, discriminator acc = 0.296\n",
      "Iteration 4799, source loss =  1.202, discriminator acc = 0.403\n",
      "Iteration 4899, source loss =  1.195, discriminator acc = 0.774\n",
      "Iteration 4999, source loss =  1.195, discriminator acc = 0.709\n",
      "Iteration 5099, source loss =  1.138, discriminator acc = 0.715\n",
      "Iteration 5199, source loss =  1.185, discriminator acc = 0.674\n",
      "Iteration 5299, source loss =  1.126, discriminator acc = 0.787\n",
      "Iteration 5399, source loss =  1.136, discriminator acc = 0.472\n",
      "Iteration 5499, source loss =  1.082, discriminator acc = 0.906\n",
      "Iteration 5599, source loss =  1.088, discriminator acc = 0.801\n",
      "Iteration 5699, source loss =  1.057, discriminator acc = 0.558\n",
      "Iteration 5799, source loss =  1.067, discriminator acc = 0.494\n",
      "Iteration 5899, source loss =  1.054, discriminator acc = 0.459\n",
      "Iteration 5999, source loss =  1.035, discriminator acc = 0.511\n",
      "Iteration 6099, source loss =  1.117, discriminator acc = 0.811\n",
      "Iteration 6199, source loss =  1.335, discriminator acc = 0.406\n",
      "Iteration 6299, source loss =  1.303, discriminator acc = 0.786\n",
      "Iteration 6399, source loss =  1.190, discriminator acc = 0.766\n",
      "Iteration 6499, source loss =  1.214, discriminator acc = 0.390\n",
      "Iteration 6599, source loss =  1.204, discriminator acc = 0.319\n",
      "Iteration 6699, source loss =  1.116, discriminator acc = 0.873\n",
      "Iteration 6799, source loss =  1.135, discriminator acc = 0.842\n",
      "Iteration 6899, source loss =  1.157, discriminator acc = 0.703\n",
      "Iteration 6999, source loss =  1.103, discriminator acc = 0.711\n",
      "Iteration 7099, source loss =  1.082, discriminator acc = 0.883\n",
      "Iteration 7199, source loss =  1.052, discriminator acc = 0.725\n",
      "Iteration 7299, source loss =  1.030, discriminator acc = 0.554\n",
      "Iteration 7399, source loss =  1.015, discriminator acc = 0.658\n",
      "Iteration 7499, source loss =  1.001, discriminator acc = 0.593\n",
      "Iteration 7599, source loss =  0.988, discriminator acc = 0.449\n",
      "Iteration 7699, source loss =  0.978, discriminator acc = 0.604\n",
      "Iteration 7799, source loss =  0.966, discriminator acc = 0.610\n",
      "Iteration 7899, source loss =  0.956, discriminator acc = 0.507\n",
      "Iteration 7999, source loss =  0.972, discriminator acc = 0.561\n",
      "Iteration 8099, source loss =  0.993, discriminator acc = 0.636\n",
      "Iteration 8199, source loss =  1.063, discriminator acc = 0.398\n",
      "Iteration 8299, source loss =  1.005, discriminator acc = 0.755\n",
      "Iteration 8399, source loss =  0.978, discriminator acc = 0.848\n",
      "Iteration 8499, source loss =  0.946, discriminator acc = 0.638\n",
      "Iteration 8599, source loss =  0.942, discriminator acc = 0.250\n",
      "Iteration 8699, source loss =  0.931, discriminator acc = 0.440\n",
      "Iteration 8799, source loss =  0.928, discriminator acc = 0.598\n",
      "Iteration 8899, source loss =  0.945, discriminator acc = 0.461\n",
      "Iteration 8999, source loss =  1.125, discriminator acc = 0.677\n",
      "Iteration 9099, source loss =  1.156, discriminator acc = 0.293\n",
      "Iteration 9199, source loss =  1.113, discriminator acc = 0.965\n",
      "Iteration 9299, source loss =  1.317, discriminator acc = 0.883\n",
      "Iteration 9399, source loss =  1.186, discriminator acc = 0.945\n",
      "Iteration 9499, source loss =  1.055, discriminator acc = 0.855\n",
      "Iteration 9599, source loss =  1.131, discriminator acc = 0.696\n",
      "Iteration 9699, source loss =  1.011, discriminator acc = 0.834\n",
      "Iteration 9799, source loss =  0.973, discriminator acc = 0.640\n",
      "Iteration 9899, source loss =  0.956, discriminator acc = 0.553\n",
      "Iteration 9999, source loss =  0.925, discriminator acc = 0.602\n",
      "Iteration 10099, source loss =  0.922, discriminator acc = 0.697\n",
      "Iteration 10199, source loss =  0.921, discriminator acc = 0.783\n",
      "Iteration 10299, source loss =  0.913, discriminator acc = 0.670\n",
      "Iteration 10399, source loss =  0.909, discriminator acc = 0.331\n",
      "Iteration 10499, source loss =  0.914, discriminator acc = 0.587\n",
      "Iteration 10599, source loss =  0.928, discriminator acc = 0.438\n",
      "Iteration 10699, source loss =  0.937, discriminator acc = 0.501\n",
      "Iteration 10799, source loss =  0.897, discriminator acc = 0.618\n",
      "Iteration 10899, source loss =  0.901, discriminator acc = 0.885\n",
      "Iteration 10999, source loss =  0.894, discriminator acc = 0.678\n",
      "Iteration 11099, source loss =  0.906, discriminator acc = 0.462\n",
      "Iteration 11199, source loss =  0.949, discriminator acc = 0.442\n",
      "Iteration 11299, source loss =  0.920, discriminator acc = 0.802\n",
      "Iteration 11399, source loss =  1.177, discriminator acc = 0.762\n",
      "Iteration 11499, source loss =  1.117, discriminator acc = 0.833\n",
      "Iteration 11599, source loss =  1.007, discriminator acc = 0.861\n",
      "Iteration 11699, source loss =  0.956, discriminator acc = 0.844\n",
      "Iteration 11799, source loss =  0.970, discriminator acc = 0.750\n",
      "Iteration 11899, source loss =  0.933, discriminator acc = 0.853\n",
      "Iteration 11999, source loss =  0.943, discriminator acc = 0.611\n",
      "Iteration 12099, source loss =  0.911, discriminator acc = 0.654\n",
      "Iteration 12199, source loss =  0.900, discriminator acc = 0.471\n",
      "Iteration 12299, source loss =  0.876, discriminator acc = 0.583\n",
      "Iteration 12399, source loss =  0.870, discriminator acc = 0.574\n",
      "Iteration 12499, source loss =  0.856, discriminator acc = 0.475\n",
      "Iteration 12599, source loss =  0.856, discriminator acc = 0.775\n",
      "Iteration 12699, source loss =  0.888, discriminator acc = 0.418\n",
      "Iteration 12799, source loss =  0.874, discriminator acc = 0.635\n",
      "Iteration 12899, source loss =  0.859, discriminator acc = 0.759\n",
      "Iteration 12999, source loss =  0.884, discriminator acc = 0.764\n",
      "Iteration 13099, source loss =  0.868, discriminator acc = 0.699\n",
      "Iteration 13199, source loss =  0.854, discriminator acc = 0.435\n",
      "Iteration 13299, source loss =  0.931, discriminator acc = 0.609\n",
      "Iteration 13399, source loss =  0.871, discriminator acc = 0.655\n",
      "Iteration 13499, source loss =  0.864, discriminator acc = 0.699\n",
      "Iteration 13599, source loss =  0.857, discriminator acc = 0.628\n",
      "Iteration 13699, source loss =  0.875, discriminator acc = 0.656\n",
      "Iteration 13799, source loss =  1.060, discriminator acc = 0.351\n",
      "Iteration 13899, source loss =  1.060, discriminator acc = 0.643\n",
      "Iteration 13999, source loss =  1.096, discriminator acc = 0.319\n",
      "Iteration 14099, source loss =  1.109, discriminator acc = 0.862\n",
      "Iteration 14199, source loss =  0.977, discriminator acc = 0.433\n",
      "Iteration 14299, source loss =  0.932, discriminator acc = 0.605\n",
      "Iteration 14399, source loss =  0.896, discriminator acc = 0.605\n",
      "Iteration 14499, source loss =  0.844, discriminator acc = 0.603\n",
      "Iteration 14599, source loss =  0.832, discriminator acc = 0.571\n",
      "Iteration 14699, source loss =  0.822, discriminator acc = 0.486\n",
      "Iteration 14799, source loss =  0.821, discriminator acc = 0.580\n",
      "Iteration 14899, source loss =  0.822, discriminator acc = 0.643\n",
      "Iteration 14999, source loss =  0.824, discriminator acc = 0.372\n",
      "PS seed: 343, model seed: 24385\n",
      "model_FINAL/CellDART_original/dlpfc/GSE144136_spatialLIBD/40markers/3mix_100000spots/minmax/gen_dlpfc_dlpfc-9356/24385\n",
      "Adversarial training for slides dict_keys(['151507', '151508', '151509', '151510', '151669', '151670', '151671', '151672', '151674', '151676']): \n",
      "Train on 100000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 1s 14us/sample - loss: 1.0781 - mae: 0.0314\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.6649 - mae: 0.0227\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.5762 - mae: 0.0207\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.5297 - mae: 0.0196\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.4928 - mae: 0.0187\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.4584 - mae: 0.0179\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.4236 - mae: 0.0170\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3895 - mae: 0.0161\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3599 - mae: 0.0153\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3313 - mae: 0.0145\n",
      "initial_train_done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42610923714637755\n",
      "0.42610923714637755\n",
      "Iteration 99, source loss =  4.460, discriminator acc = 0.763\n",
      "Iteration 199, source loss =  3.640, discriminator acc = 0.989\n",
      "Iteration 299, source loss =  2.021, discriminator acc = 0.971\n",
      "Iteration 399, source loss =  1.832, discriminator acc = 0.414\n",
      "Iteration 499, source loss =  1.908, discriminator acc = 0.943\n",
      "Iteration 599, source loss =  1.840, discriminator acc = 0.690\n",
      "Iteration 699, source loss =  1.792, discriminator acc = 0.961\n",
      "Iteration 799, source loss =  1.839, discriminator acc = 0.828\n",
      "Iteration 899, source loss =  1.806, discriminator acc = 0.914\n",
      "Iteration 999, source loss =  1.495, discriminator acc = 0.937\n",
      "Iteration 1099, source loss =  1.408, discriminator acc = 0.978\n",
      "Iteration 1199, source loss =  1.547, discriminator acc = 0.951\n",
      "Iteration 1299, source loss =  1.454, discriminator acc = 0.952\n",
      "Iteration 1399, source loss =  1.459, discriminator acc = 0.438\n",
      "Iteration 1499, source loss =  1.428, discriminator acc = 0.682\n",
      "Iteration 1599, source loss =  1.442, discriminator acc = 0.528\n",
      "Iteration 1699, source loss =  1.485, discriminator acc = 0.747\n",
      "Iteration 1799, source loss =  1.519, discriminator acc = 0.913\n",
      "Iteration 1899, source loss =  1.732, discriminator acc = 0.337\n",
      "Iteration 1999, source loss =  1.662, discriminator acc = 0.461\n",
      "Iteration 2099, source loss =  1.682, discriminator acc = 0.846\n",
      "Iteration 2199, source loss =  1.677, discriminator acc = 0.977\n",
      "Iteration 2299, source loss =  1.628, discriminator acc = 0.829\n",
      "Iteration 2399, source loss =  1.677, discriminator acc = 0.647\n",
      "Iteration 2499, source loss =  1.559, discriminator acc = 0.890\n",
      "Iteration 2599, source loss =  1.638, discriminator acc = 0.884\n",
      "Iteration 2699, source loss =  1.613, discriminator acc = 0.975\n",
      "Iteration 2799, source loss =  1.589, discriminator acc = 0.947\n",
      "Iteration 2899, source loss =  1.449, discriminator acc = 0.974\n",
      "Iteration 2999, source loss =  1.481, discriminator acc = 0.531\n",
      "Iteration 3099, source loss =  1.410, discriminator acc = 0.636\n",
      "Iteration 3199, source loss =  1.380, discriminator acc = 0.685\n",
      "Iteration 3299, source loss =  1.357, discriminator acc = 0.604\n",
      "Iteration 3399, source loss =  1.353, discriminator acc = 0.578\n",
      "Iteration 3499, source loss =  1.332, discriminator acc = 0.707\n",
      "Iteration 3599, source loss =  1.330, discriminator acc = 0.656\n",
      "Iteration 3699, source loss =  1.351, discriminator acc = 0.891\n",
      "Iteration 3799, source loss =  1.334, discriminator acc = 0.832\n",
      "Iteration 3899, source loss =  1.447, discriminator acc = 0.668\n",
      "Iteration 3999, source loss =  1.331, discriminator acc = 0.922\n",
      "Iteration 4099, source loss =  1.323, discriminator acc = 0.513\n",
      "Iteration 4199, source loss =  1.287, discriminator acc = 0.837\n",
      "Iteration 4299, source loss =  1.283, discriminator acc = 0.463\n",
      "Iteration 4399, source loss =  1.271, discriminator acc = 0.911\n",
      "Iteration 4499, source loss =  1.258, discriminator acc = 0.868\n",
      "Iteration 4599, source loss =  1.291, discriminator acc = 0.590\n",
      "Iteration 4699, source loss =  1.288, discriminator acc = 0.655\n",
      "Iteration 4799, source loss =  1.255, discriminator acc = 0.746\n",
      "Iteration 4899, source loss =  1.268, discriminator acc = 0.647\n",
      "Iteration 4999, source loss =  1.260, discriminator acc = 0.440\n",
      "Iteration 5099, source loss =  1.207, discriminator acc = 0.587\n",
      "Iteration 5199, source loss =  1.253, discriminator acc = 0.614\n",
      "Iteration 5299, source loss =  1.197, discriminator acc = 0.713\n",
      "Iteration 5399, source loss =  1.226, discriminator acc = 0.680\n",
      "Iteration 5499, source loss =  1.307, discriminator acc = 0.802\n",
      "Iteration 5599, source loss =  1.216, discriminator acc = 0.564\n",
      "Iteration 5699, source loss =  1.202, discriminator acc = 0.658\n",
      "Iteration 5799, source loss =  1.200, discriminator acc = 0.510\n",
      "Iteration 5899, source loss =  1.141, discriminator acc = 0.464\n",
      "Iteration 5999, source loss =  1.153, discriminator acc = 0.424\n",
      "Iteration 6099, source loss =  1.126, discriminator acc = 0.797\n",
      "Iteration 6199, source loss =  1.133, discriminator acc = 0.571\n",
      "Iteration 6299, source loss =  1.158, discriminator acc = 0.802\n",
      "Iteration 6399, source loss =  1.114, discriminator acc = 0.578\n",
      "Iteration 6499, source loss =  1.114, discriminator acc = 0.467\n",
      "Iteration 6599, source loss =  1.121, discriminator acc = 0.587\n",
      "Iteration 6699, source loss =  1.080, discriminator acc = 0.683\n",
      "Iteration 6799, source loss =  1.112, discriminator acc = 0.549\n",
      "Iteration 6899, source loss =  1.098, discriminator acc = 0.845\n",
      "Iteration 6999, source loss =  1.079, discriminator acc = 0.853\n",
      "Iteration 7099, source loss =  1.358, discriminator acc = 0.514\n",
      "Iteration 7199, source loss =  1.230, discriminator acc = 0.669\n",
      "Iteration 7299, source loss =  1.113, discriminator acc = 0.765\n",
      "Iteration 7399, source loss =  1.047, discriminator acc = 0.697\n",
      "Iteration 7499, source loss =  1.034, discriminator acc = 0.670\n",
      "Iteration 7599, source loss =  1.011, discriminator acc = 0.611\n",
      "Iteration 7699, source loss =  0.998, discriminator acc = 0.619\n",
      "Iteration 7799, source loss =  1.001, discriminator acc = 0.533\n",
      "Iteration 7899, source loss =  0.983, discriminator acc = 0.690\n",
      "Iteration 7999, source loss =  1.139, discriminator acc = 0.369\n",
      "Iteration 8099, source loss =  1.095, discriminator acc = 0.775\n",
      "Iteration 8199, source loss =  1.039, discriminator acc = 0.755\n",
      "Iteration 8299, source loss =  1.201, discriminator acc = 0.380\n",
      "Iteration 8399, source loss =  1.286, discriminator acc = 0.256\n",
      "Iteration 8499, source loss =  1.133, discriminator acc = 0.941\n",
      "Iteration 8599, source loss =  1.126, discriminator acc = 0.703\n",
      "Iteration 8699, source loss =  1.070, discriminator acc = 0.428\n",
      "Iteration 8799, source loss =  1.021, discriminator acc = 0.711\n",
      "Iteration 8899, source loss =  0.996, discriminator acc = 0.902\n",
      "Iteration 8999, source loss =  1.019, discriminator acc = 0.788\n",
      "Iteration 9099, source loss =  0.977, discriminator acc = 0.377\n",
      "Iteration 9199, source loss =  0.960, discriminator acc = 0.470\n",
      "Iteration 9299, source loss =  0.976, discriminator acc = 0.646\n",
      "Iteration 9399, source loss =  1.043, discriminator acc = 0.486\n",
      "Iteration 9499, source loss =  0.934, discriminator acc = 0.778\n",
      "Iteration 9599, source loss =  0.927, discriminator acc = 0.445\n",
      "Iteration 9699, source loss =  0.913, discriminator acc = 0.826\n",
      "Iteration 9799, source loss =  0.909, discriminator acc = 0.728\n",
      "Iteration 9899, source loss =  0.926, discriminator acc = 0.558\n",
      "Iteration 9999, source loss =  1.059, discriminator acc = 0.418\n",
      "Iteration 10099, source loss =  1.020, discriminator acc = 0.640\n",
      "Iteration 10199, source loss =  1.212, discriminator acc = 0.345\n",
      "Iteration 10299, source loss =  1.115, discriminator acc = 0.778\n",
      "Iteration 10399, source loss =  1.016, discriminator acc = 0.624\n",
      "Iteration 10499, source loss =  0.927, discriminator acc = 0.537\n",
      "Iteration 10599, source loss =  0.935, discriminator acc = 0.562\n",
      "Iteration 10699, source loss =  0.935, discriminator acc = 0.849\n",
      "Iteration 10799, source loss =  1.008, discriminator acc = 0.681\n",
      "Iteration 10899, source loss =  0.907, discriminator acc = 0.837\n",
      "Iteration 10999, source loss =  0.926, discriminator acc = 0.702\n",
      "Iteration 11099, source loss =  0.904, discriminator acc = 0.667\n",
      "Iteration 11199, source loss =  0.955, discriminator acc = 0.650\n",
      "Iteration 11299, source loss =  0.883, discriminator acc = 0.694\n",
      "Iteration 11399, source loss =  0.878, discriminator acc = 0.706\n",
      "Iteration 11499, source loss =  0.909, discriminator acc = 0.331\n",
      "Iteration 11599, source loss =  0.875, discriminator acc = 0.675\n",
      "Iteration 11699, source loss =  0.912, discriminator acc = 0.691\n",
      "Iteration 11799, source loss =  0.908, discriminator acc = 0.717\n",
      "Iteration 11899, source loss =  0.885, discriminator acc = 0.760\n",
      "Iteration 11999, source loss =  0.964, discriminator acc = 0.910\n",
      "Iteration 12099, source loss =  1.313, discriminator acc = 0.808\n",
      "Iteration 12199, source loss =  1.095, discriminator acc = 0.294\n",
      "Iteration 12299, source loss =  1.017, discriminator acc = 0.480\n",
      "Iteration 12399, source loss =  0.985, discriminator acc = 0.436\n",
      "Iteration 12499, source loss =  1.016, discriminator acc = 0.807\n",
      "Iteration 12599, source loss =  0.950, discriminator acc = 0.292\n",
      "Iteration 12699, source loss =  0.885, discriminator acc = 0.612\n",
      "Iteration 12799, source loss =  0.881, discriminator acc = 0.635\n",
      "Iteration 12899, source loss =  0.878, discriminator acc = 0.713\n",
      "Iteration 12999, source loss =  0.922, discriminator acc = 0.431\n",
      "Iteration 13099, source loss =  0.867, discriminator acc = 0.601\n",
      "Iteration 13199, source loss =  0.861, discriminator acc = 0.572\n",
      "Iteration 13299, source loss =  0.903, discriminator acc = 0.591\n",
      "Iteration 13399, source loss =  0.901, discriminator acc = 0.536\n",
      "Iteration 13499, source loss =  0.881, discriminator acc = 0.651\n",
      "Iteration 13599, source loss =  0.914, discriminator acc = 0.420\n",
      "Iteration 13699, source loss =  0.999, discriminator acc = 0.368\n",
      "Iteration 13799, source loss =  0.988, discriminator acc = 0.829\n",
      "Iteration 13899, source loss =  0.957, discriminator acc = 0.586\n",
      "Iteration 13999, source loss =  0.907, discriminator acc = 0.651\n",
      "Iteration 14099, source loss =  1.035, discriminator acc = 0.372\n",
      "Iteration 14199, source loss =  0.880, discriminator acc = 0.750\n",
      "Iteration 14299, source loss =  1.017, discriminator acc = 0.496\n",
      "Iteration 14399, source loss =  0.925, discriminator acc = 0.768\n",
      "Iteration 14499, source loss =  0.898, discriminator acc = 0.486\n",
      "Iteration 14599, source loss =  0.871, discriminator acc = 0.694\n",
      "Iteration 14699, source loss =  0.864, discriminator acc = 0.391\n",
      "Iteration 14799, source loss =  0.853, discriminator acc = 0.420\n",
      "Iteration 14899, source loss =  0.844, discriminator acc = 0.472\n",
      "Iteration 14999, source loss =  0.860, discriminator acc = 0.403\n",
      "PS seed: 25, model seed: 284\n",
      "model_FINAL/CellDART_original/dlpfc/GSE144136_spatialLIBD/40markers/3mix_100000spots/minmax/gen_dlpfc_dlpfc-9356/284\n",
      "Adversarial training for slides dict_keys(['151507', '151508', '151509', '151510', '151669', '151670', '151671', '151672', '151674', '151676']): \n",
      "Train on 100000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 1s 14us/sample - loss: 1.0807 - mae: 0.0317\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.6674 - mae: 0.0229\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.5782 - mae: 0.0208\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.5317 - mae: 0.0197\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.4945 - mae: 0.0188\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.4582 - mae: 0.0179\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.4247 - mae: 0.0171\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3898 - mae: 0.0161\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3594 - mae: 0.0153\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.3323 - mae: 0.0146\n",
      "initial_train_done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4182668998718262\n",
      "0.4182668998718262\n",
      "Iteration 99, source loss =  4.476, discriminator acc = 0.904\n",
      "Iteration 199, source loss =  3.855, discriminator acc = 0.915\n",
      "Iteration 299, source loss =  3.103, discriminator acc = 0.956\n",
      "Iteration 399, source loss =  2.381, discriminator acc = 0.971\n",
      "Iteration 499, source loss =  1.750, discriminator acc = 0.913\n",
      "Iteration 599, source loss =  1.882, discriminator acc = 0.971\n",
      "Iteration 699, source loss =  1.830, discriminator acc = 0.598\n",
      "Iteration 799, source loss =  1.527, discriminator acc = 0.330\n",
      "Iteration 899, source loss =  1.563, discriminator acc = 0.957\n",
      "Iteration 999, source loss =  1.634, discriminator acc = 0.841\n",
      "Iteration 1099, source loss =  1.763, discriminator acc = 0.456\n",
      "Iteration 1199, source loss =  1.630, discriminator acc = 0.733\n",
      "Iteration 1299, source loss =  2.009, discriminator acc = 0.988\n",
      "Iteration 1399, source loss =  2.047, discriminator acc = 0.911\n",
      "Iteration 1499, source loss =  1.693, discriminator acc = 0.904\n",
      "Iteration 1599, source loss =  1.727, discriminator acc = 0.909\n",
      "Iteration 1699, source loss =  1.692, discriminator acc = 0.998\n",
      "Iteration 1799, source loss =  1.638, discriminator acc = 0.824\n",
      "Iteration 1899, source loss =  1.543, discriminator acc = 0.954\n",
      "Iteration 1999, source loss =  1.456, discriminator acc = 0.977\n",
      "Iteration 2099, source loss =  1.397, discriminator acc = 0.900\n",
      "Iteration 2199, source loss =  1.489, discriminator acc = 0.447\n",
      "Iteration 2299, source loss =  1.345, discriminator acc = 0.799\n",
      "Iteration 2399, source loss =  1.355, discriminator acc = 0.628\n",
      "Iteration 2499, source loss =  1.315, discriminator acc = 0.761\n",
      "Iteration 2599, source loss =  1.301, discriminator acc = 0.447\n",
      "Iteration 2699, source loss =  1.301, discriminator acc = 0.849\n",
      "Iteration 2799, source loss =  1.315, discriminator acc = 0.766\n",
      "Iteration 2899, source loss =  1.309, discriminator acc = 0.840\n",
      "Iteration 2999, source loss =  1.297, discriminator acc = 0.605\n",
      "Iteration 3099, source loss =  1.314, discriminator acc = 0.668\n",
      "Iteration 3199, source loss =  1.340, discriminator acc = 0.701\n",
      "Iteration 3299, source loss =  1.312, discriminator acc = 0.436\n",
      "Iteration 3399, source loss =  1.296, discriminator acc = 0.895\n",
      "Iteration 3499, source loss =  1.261, discriminator acc = 0.912\n",
      "Iteration 3599, source loss =  1.305, discriminator acc = 0.889\n",
      "Iteration 3699, source loss =  1.352, discriminator acc = 0.817\n",
      "Iteration 3799, source loss =  1.261, discriminator acc = 0.599\n",
      "Iteration 3899, source loss =  1.300, discriminator acc = 0.827\n",
      "Iteration 3999, source loss =  1.284, discriminator acc = 0.582\n",
      "Iteration 4099, source loss =  1.225, discriminator acc = 0.815\n",
      "Iteration 4199, source loss =  1.356, discriminator acc = 0.298\n",
      "Iteration 4299, source loss =  1.206, discriminator acc = 0.836\n",
      "Iteration 4399, source loss =  1.214, discriminator acc = 0.699\n",
      "Iteration 4499, source loss =  1.221, discriminator acc = 0.574\n",
      "Iteration 4599, source loss =  1.163, discriminator acc = 0.668\n",
      "Iteration 4699, source loss =  1.189, discriminator acc = 0.402\n",
      "Iteration 4799, source loss =  1.226, discriminator acc = 0.539\n",
      "Iteration 4899, source loss =  1.164, discriminator acc = 0.853\n",
      "Iteration 4999, source loss =  1.162, discriminator acc = 0.590\n",
      "Iteration 5099, source loss =  1.230, discriminator acc = 0.796\n",
      "Iteration 5199, source loss =  1.164, discriminator acc = 0.572\n",
      "Iteration 5299, source loss =  1.141, discriminator acc = 0.622\n",
      "Iteration 5399, source loss =  1.117, discriminator acc = 0.600\n",
      "Iteration 5499, source loss =  1.198, discriminator acc = 0.621\n",
      "Iteration 5599, source loss =  1.220, discriminator acc = 0.651\n",
      "Iteration 5699, source loss =  1.110, discriminator acc = 0.603\n",
      "Iteration 5799, source loss =  1.056, discriminator acc = 0.667\n",
      "Iteration 5899, source loss =  1.035, discriminator acc = 0.687\n",
      "Iteration 5999, source loss =  1.125, discriminator acc = 0.656\n",
      "Iteration 6099, source loss =  1.056, discriminator acc = 0.486\n",
      "Iteration 6199, source loss =  1.063, discriminator acc = 0.402\n",
      "Iteration 6299, source loss =  1.066, discriminator acc = 0.827\n",
      "Iteration 6399, source loss =  1.098, discriminator acc = 0.432\n",
      "Iteration 6499, source loss =  1.030, discriminator acc = 0.889\n",
      "Iteration 6599, source loss =  1.024, discriminator acc = 0.614\n",
      "Iteration 6699, source loss =  1.025, discriminator acc = 0.702\n",
      "Iteration 6799, source loss =  1.123, discriminator acc = 0.725\n",
      "Iteration 6899, source loss =  1.320, discriminator acc = 0.419\n",
      "Iteration 6999, source loss =  1.235, discriminator acc = 0.885\n",
      "Iteration 7099, source loss =  1.261, discriminator acc = 0.670\n",
      "Iteration 7199, source loss =  1.284, discriminator acc = 0.834\n",
      "Iteration 7299, source loss =  1.173, discriminator acc = 0.788\n",
      "Iteration 7399, source loss =  1.135, discriminator acc = 0.690\n",
      "Iteration 7499, source loss =  1.110, discriminator acc = 0.469\n",
      "Iteration 7599, source loss =  1.045, discriminator acc = 0.768\n",
      "Iteration 7699, source loss =  1.004, discriminator acc = 0.586\n",
      "Iteration 7799, source loss =  0.998, discriminator acc = 0.488\n",
      "Iteration 7899, source loss =  0.971, discriminator acc = 0.600\n",
      "Iteration 7999, source loss =  1.005, discriminator acc = 0.464\n",
      "Iteration 8099, source loss =  1.031, discriminator acc = 0.810\n",
      "Iteration 8199, source loss =  0.967, discriminator acc = 0.772\n",
      "Iteration 8299, source loss =  0.969, discriminator acc = 0.472\n",
      "Iteration 8399, source loss =  0.984, discriminator acc = 0.725\n",
      "Iteration 8499, source loss =  0.967, discriminator acc = 0.712\n",
      "Iteration 8599, source loss =  1.079, discriminator acc = 0.577\n",
      "Iteration 8699, source loss =  1.043, discriminator acc = 0.846\n",
      "Iteration 8799, source loss =  1.021, discriminator acc = 0.472\n",
      "Iteration 8899, source loss =  1.014, discriminator acc = 0.499\n",
      "Iteration 8999, source loss =  0.976, discriminator acc = 0.600\n",
      "Iteration 9099, source loss =  0.985, discriminator acc = 0.356\n",
      "Iteration 9199, source loss =  0.982, discriminator acc = 0.819\n",
      "Iteration 9299, source loss =  0.968, discriminator acc = 0.781\n",
      "Iteration 9399, source loss =  1.023, discriminator acc = 0.658\n",
      "Iteration 9499, source loss =  1.004, discriminator acc = 0.737\n",
      "Iteration 9599, source loss =  1.114, discriminator acc = 0.833\n",
      "Iteration 9699, source loss =  1.010, discriminator acc = 0.345\n",
      "Iteration 9799, source loss =  0.961, discriminator acc = 0.560\n",
      "Iteration 9899, source loss =  0.931, discriminator acc = 0.469\n",
      "Iteration 9999, source loss =  0.907, discriminator acc = 0.668\n",
      "Iteration 10099, source loss =  0.893, discriminator acc = 0.589\n",
      "Iteration 10199, source loss =  0.888, discriminator acc = 0.451\n",
      "Iteration 10299, source loss =  0.870, discriminator acc = 0.521\n",
      "Iteration 10399, source loss =  0.866, discriminator acc = 0.627\n",
      "Iteration 10499, source loss =  0.892, discriminator acc = 0.426\n",
      "Iteration 10599, source loss =  0.868, discriminator acc = 0.683\n",
      "Iteration 10699, source loss =  0.948, discriminator acc = 0.664\n",
      "Iteration 10799, source loss =  0.988, discriminator acc = 0.711\n",
      "Iteration 10899, source loss =  1.139, discriminator acc = 0.942\n",
      "Iteration 10999, source loss =  1.203, discriminator acc = 0.899\n",
      "Iteration 11099, source loss =  1.143, discriminator acc = 0.470\n",
      "Iteration 11199, source loss =  1.285, discriminator acc = 0.545\n",
      "Iteration 11299, source loss =  1.104, discriminator acc = 0.570\n",
      "Iteration 11399, source loss =  1.044, discriminator acc = 0.428\n",
      "Iteration 11499, source loss =  1.013, discriminator acc = 0.705\n",
      "Iteration 11599, source loss =  0.967, discriminator acc = 0.576\n",
      "Iteration 11699, source loss =  0.930, discriminator acc = 0.435\n",
      "Iteration 11799, source loss =  0.909, discriminator acc = 0.581\n",
      "Iteration 11899, source loss =  0.892, discriminator acc = 0.573\n",
      "Iteration 11999, source loss =  0.893, discriminator acc = 0.608\n",
      "Iteration 12099, source loss =  0.926, discriminator acc = 0.528\n",
      "Iteration 12199, source loss =  0.913, discriminator acc = 0.394\n",
      "Iteration 12299, source loss =  0.856, discriminator acc = 0.587\n",
      "Iteration 12399, source loss =  0.857, discriminator acc = 0.588\n",
      "Iteration 12499, source loss =  0.868, discriminator acc = 0.681\n",
      "Iteration 12599, source loss =  0.876, discriminator acc = 0.709\n",
      "Iteration 12699, source loss =  0.893, discriminator acc = 0.693\n",
      "Iteration 12799, source loss =  0.901, discriminator acc = 0.637\n",
      "Iteration 12899, source loss =  0.874, discriminator acc = 0.545\n",
      "Iteration 12999, source loss =  0.897, discriminator acc = 0.495\n",
      "Iteration 13099, source loss =  0.847, discriminator acc = 0.631\n",
      "Iteration 13199, source loss =  0.851, discriminator acc = 0.623\n",
      "Iteration 13299, source loss =  0.868, discriminator acc = 0.928\n",
      "Iteration 13399, source loss =  0.884, discriminator acc = 0.339\n",
      "Iteration 13499, source loss =  0.969, discriminator acc = 0.388\n",
      "Iteration 13599, source loss =  1.069, discriminator acc = 0.915\n",
      "Iteration 13699, source loss =  1.008, discriminator acc = 0.482\n",
      "Iteration 13799, source loss =  1.258, discriminator acc = 0.159\n",
      "Iteration 13899, source loss =  0.943, discriminator acc = 0.890\n",
      "Iteration 13999, source loss =  0.905, discriminator acc = 0.889\n",
      "Iteration 14099, source loss =  0.900, discriminator acc = 0.384\n",
      "Iteration 14199, source loss =  0.868, discriminator acc = 0.675\n",
      "Iteration 14299, source loss =  0.864, discriminator acc = 0.630\n",
      "Iteration 14399, source loss =  0.845, discriminator acc = 0.845\n",
      "Iteration 14499, source loss =  0.886, discriminator acc = 0.581\n",
      "Iteration 14599, source loss =  0.875, discriminator acc = 0.535\n",
      "Iteration 14699, source loss =  0.831, discriminator acc = 0.683\n",
      "Iteration 14799, source loss =  0.830, discriminator acc = 0.642\n",
      "Iteration 14899, source loss =  0.833, discriminator acc = 0.785\n",
      "Iteration 14999, source loss =  0.901, discriminator acc = 0.711\n",
      "PS seed: 234, model seed: 86322\n",
      "model_FINAL/CellDART_original/dlpfc/GSE144136_spatialLIBD/40markers/3mix_100000spots/minmax/gen_dlpfc_dlpfc-9356/86322\n",
      "Adversarial training for slides dict_keys(['151507', '151508', '151509', '151510', '151669', '151670', '151671', '151672', '151674', '151676']): \n",
      "Train on 100000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 1s 14us/sample - loss: 1.0204 - mae: 0.0306\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 1s 11us/sample - loss: 0.6482 - mae: 0.0224\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.5738 - mae: 0.0206\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.5300 - mae: 0.0196\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.4923 - mae: 0.0187\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 2s 16us/sample - loss: 0.4572 - mae: 0.0178\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.4222 - mae: 0.0169\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.3886 - mae: 0.0161\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.3584 - mae: 0.0153\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 1s 13us/sample - loss: 0.3306 - mae: 0.0145\n",
      "initial_train_done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3818477215671539\n",
      "0.3818477215671539\n",
      "Iteration 99, source loss =  3.642, discriminator acc = 0.993\n",
      "Iteration 199, source loss =  3.271, discriminator acc = 0.741\n",
      "Iteration 299, source loss =  2.383, discriminator acc = 0.576\n",
      "Iteration 399, source loss =  2.077, discriminator acc = 0.997\n",
      "Iteration 499, source loss =  1.914, discriminator acc = 0.492\n",
      "Iteration 599, source loss =  1.490, discriminator acc = 0.880\n",
      "Iteration 699, source loss =  1.610, discriminator acc = 0.813\n",
      "Iteration 799, source loss =  1.528, discriminator acc = 0.621\n",
      "Iteration 899, source loss =  1.287, discriminator acc = 0.708\n",
      "Iteration 999, source loss =  1.160, discriminator acc = 0.882\n",
      "Iteration 1099, source loss =  1.317, discriminator acc = 0.423\n",
      "Iteration 1199, source loss =  1.913, discriminator acc = 0.773\n",
      "Iteration 1299, source loss =  2.125, discriminator acc = 0.318\n",
      "Iteration 1399, source loss =  1.518, discriminator acc = 0.588\n",
      "Iteration 1499, source loss =  1.465, discriminator acc = 0.601\n",
      "Iteration 1599, source loss =  1.434, discriminator acc = 0.926\n",
      "Iteration 1699, source loss =  1.285, discriminator acc = 0.632\n",
      "Iteration 1799, source loss =  1.405, discriminator acc = 0.352\n",
      "Iteration 1899, source loss =  1.291, discriminator acc = 0.466\n",
      "Iteration 1999, source loss =  1.583, discriminator acc = 0.838\n",
      "Iteration 2099, source loss =  1.548, discriminator acc = 0.737\n",
      "Iteration 2199, source loss =  1.535, discriminator acc = 0.790\n",
      "Iteration 2299, source loss =  1.785, discriminator acc = 0.953\n",
      "Iteration 2399, source loss =  1.815, discriminator acc = 0.987\n",
      "Iteration 2499, source loss =  1.460, discriminator acc = 0.988\n",
      "Iteration 2599, source loss =  1.650, discriminator acc = 0.342\n",
      "Iteration 2699, source loss =  1.427, discriminator acc = 0.646\n",
      "Iteration 2799, source loss =  1.474, discriminator acc = 0.432\n",
      "Iteration 2899, source loss =  1.466, discriminator acc = 0.701\n",
      "Iteration 2999, source loss =  1.288, discriminator acc = 0.944\n",
      "Iteration 3099, source loss =  1.300, discriminator acc = 0.945\n",
      "Iteration 3199, source loss =  1.341, discriminator acc = 0.767\n",
      "Iteration 3299, source loss =  1.275, discriminator acc = 0.628\n",
      "Iteration 3399, source loss =  1.420, discriminator acc = 0.725\n",
      "Iteration 3499, source loss =  1.390, discriminator acc = 0.439\n",
      "Iteration 3599, source loss =  1.319, discriminator acc = 0.550\n",
      "Iteration 3699, source loss =  1.331, discriminator acc = 0.828\n",
      "Iteration 3799, source loss =  1.309, discriminator acc = 0.717\n",
      "Iteration 3899, source loss =  1.274, discriminator acc = 0.781\n",
      "Iteration 3999, source loss =  1.240, discriminator acc = 0.730\n",
      "Iteration 4099, source loss =  1.275, discriminator acc = 0.424\n",
      "Iteration 4199, source loss =  1.245, discriminator acc = 0.614\n",
      "Iteration 4299, source loss =  1.223, discriminator acc = 0.631\n",
      "Iteration 4399, source loss =  1.191, discriminator acc = 0.597\n",
      "Iteration 4499, source loss =  1.209, discriminator acc = 0.824\n",
      "Iteration 4599, source loss =  1.279, discriminator acc = 0.351\n",
      "Iteration 4699, source loss =  1.219, discriminator acc = 0.904\n",
      "Iteration 4799, source loss =  1.201, discriminator acc = 0.674\n",
      "Iteration 4899, source loss =  1.255, discriminator acc = 0.837\n",
      "Iteration 4999, source loss =  1.220, discriminator acc = 0.602\n",
      "Iteration 5099, source loss =  1.168, discriminator acc = 0.694\n",
      "Iteration 5199, source loss =  1.136, discriminator acc = 0.594\n",
      "Iteration 5299, source loss =  1.147, discriminator acc = 0.470\n",
      "Iteration 5399, source loss =  1.237, discriminator acc = 0.357\n",
      "Iteration 5499, source loss =  1.198, discriminator acc = 0.563\n",
      "Iteration 5599, source loss =  1.346, discriminator acc = 0.789\n",
      "Iteration 5699, source loss =  1.201, discriminator acc = 0.683\n",
      "Iteration 5799, source loss =  1.256, discriminator acc = 0.833\n",
      "Iteration 5899, source loss =  1.223, discriminator acc = 0.798\n",
      "Iteration 5999, source loss =  1.161, discriminator acc = 0.538\n",
      "Iteration 6099, source loss =  1.200, discriminator acc = 0.524\n",
      "Iteration 6199, source loss =  1.118, discriminator acc = 0.736\n",
      "Iteration 6299, source loss =  1.140, discriminator acc = 0.447\n",
      "Iteration 6399, source loss =  1.092, discriminator acc = 0.450\n",
      "Iteration 6499, source loss =  1.062, discriminator acc = 0.536\n",
      "Iteration 6599, source loss =  1.039, discriminator acc = 0.468\n",
      "Iteration 6699, source loss =  1.029, discriminator acc = 0.426\n",
      "Iteration 6799, source loss =  1.070, discriminator acc = 0.615\n",
      "Iteration 6899, source loss =  1.056, discriminator acc = 0.421\n",
      "Iteration 6999, source loss =  1.037, discriminator acc = 0.588\n",
      "Iteration 7099, source loss =  1.071, discriminator acc = 0.305\n",
      "Iteration 7199, source loss =  1.067, discriminator acc = 0.403\n",
      "Iteration 7299, source loss =  1.049, discriminator acc = 0.623\n",
      "Iteration 7399, source loss =  1.058, discriminator acc = 0.899\n",
      "Iteration 7499, source loss =  1.046, discriminator acc = 0.616\n",
      "Iteration 7599, source loss =  1.112, discriminator acc = 0.296\n",
      "Iteration 7699, source loss =  1.067, discriminator acc = 0.394\n",
      "Iteration 7799, source loss =  1.024, discriminator acc = 0.826\n",
      "Iteration 7899, source loss =  1.029, discriminator acc = 0.525\n",
      "Iteration 7999, source loss =  1.036, discriminator acc = 0.951\n",
      "Iteration 8099, source loss =  1.066, discriminator acc = 0.552\n",
      "Iteration 8199, source loss =  1.044, discriminator acc = 0.685\n",
      "Iteration 8299, source loss =  1.078, discriminator acc = 0.686\n",
      "Iteration 8399, source loss =  1.005, discriminator acc = 0.541\n",
      "Iteration 8499, source loss =  0.997, discriminator acc = 0.457\n",
      "Iteration 8599, source loss =  0.989, discriminator acc = 0.931\n",
      "Iteration 8699, source loss =  1.115, discriminator acc = 0.596\n",
      "Iteration 8799, source loss =  1.163, discriminator acc = 0.416\n",
      "Iteration 8899, source loss =  1.122, discriminator acc = 0.794\n",
      "Iteration 8999, source loss =  1.049, discriminator acc = 0.832\n",
      "Iteration 9099, source loss =  1.082, discriminator acc = 0.592\n",
      "Iteration 9199, source loss =  1.038, discriminator acc = 0.552\n",
      "Iteration 9299, source loss =  0.971, discriminator acc = 0.610\n",
      "Iteration 9399, source loss =  0.924, discriminator acc = 0.646\n",
      "Iteration 9499, source loss =  0.933, discriminator acc = 0.636\n",
      "Iteration 9599, source loss =  1.000, discriminator acc = 0.456\n",
      "Iteration 9699, source loss =  1.009, discriminator acc = 0.460\n",
      "Iteration 9799, source loss =  0.974, discriminator acc = 0.626\n",
      "Iteration 9899, source loss =  0.941, discriminator acc = 0.922\n",
      "Iteration 9999, source loss =  0.932, discriminator acc = 0.511\n",
      "Iteration 10099, source loss =  0.937, discriminator acc = 0.481\n",
      "Iteration 10199, source loss =  0.899, discriminator acc = 0.508\n",
      "Iteration 10299, source loss =  0.910, discriminator acc = 0.710\n",
      "Iteration 10399, source loss =  0.913, discriminator acc = 0.483\n",
      "Iteration 10499, source loss =  0.971, discriminator acc = 0.726\n",
      "Iteration 10599, source loss =  1.156, discriminator acc = 0.647\n",
      "Iteration 10699, source loss =  1.219, discriminator acc = 0.583\n",
      "Iteration 10799, source loss =  1.041, discriminator acc = 0.941\n",
      "Iteration 10899, source loss =  1.131, discriminator acc = 0.776\n",
      "Iteration 10999, source loss =  1.003, discriminator acc = 0.876\n",
      "Iteration 11099, source loss =  1.024, discriminator acc = 0.553\n",
      "Iteration 11199, source loss =  0.970, discriminator acc = 0.829\n",
      "Iteration 11299, source loss =  0.951, discriminator acc = 0.477\n",
      "Iteration 11399, source loss =  0.971, discriminator acc = 0.256\n",
      "Iteration 11499, source loss =  0.884, discriminator acc = 0.753\n",
      "Iteration 11599, source loss =  0.874, discriminator acc = 0.678\n",
      "Iteration 11699, source loss =  0.886, discriminator acc = 0.415\n",
      "Iteration 11799, source loss =  0.874, discriminator acc = 0.765\n",
      "Iteration 11899, source loss =  0.879, discriminator acc = 0.466\n",
      "Iteration 11999, source loss =  0.909, discriminator acc = 0.358\n",
      "Iteration 12099, source loss =  0.890, discriminator acc = 0.694\n",
      "Iteration 12199, source loss =  0.873, discriminator acc = 0.637\n",
      "Iteration 12299, source loss =  0.866, discriminator acc = 0.703\n",
      "Iteration 12399, source loss =  0.849, discriminator acc = 0.654\n",
      "Iteration 12499, source loss =  0.906, discriminator acc = 0.490\n",
      "Iteration 12599, source loss =  0.914, discriminator acc = 0.374\n",
      "Iteration 12699, source loss =  0.904, discriminator acc = 0.675\n",
      "Iteration 12799, source loss =  1.044, discriminator acc = 0.221\n",
      "Iteration 12899, source loss =  1.116, discriminator acc = 0.707\n",
      "Iteration 12999, source loss =  1.199, discriminator acc = 0.530\n",
      "Iteration 13099, source loss =  1.211, discriminator acc = 0.699\n",
      "Iteration 13199, source loss =  1.099, discriminator acc = 0.934\n",
      "Iteration 13299, source loss =  1.147, discriminator acc = 0.536\n",
      "Iteration 13399, source loss =  1.004, discriminator acc = 0.491\n",
      "Iteration 13499, source loss =  0.966, discriminator acc = 0.876\n",
      "Iteration 13599, source loss =  0.901, discriminator acc = 0.570\n",
      "Iteration 13699, source loss =  0.871, discriminator acc = 0.519\n",
      "Iteration 13799, source loss =  0.878, discriminator acc = 0.538\n",
      "Iteration 13899, source loss =  0.875, discriminator acc = 0.477\n",
      "Iteration 13999, source loss =  0.862, discriminator acc = 0.506\n",
      "Iteration 14099, source loss =  0.851, discriminator acc = 0.483\n",
      "Iteration 14199, source loss =  0.850, discriminator acc = 0.924\n",
      "Iteration 14299, source loss =  0.855, discriminator acc = 0.613\n",
      "Iteration 14399, source loss =  0.923, discriminator acc = 0.440\n",
      "Iteration 14499, source loss =  0.899, discriminator acc = 0.753\n",
      "Iteration 14599, source loss =  0.940, discriminator acc = 0.566\n",
      "Iteration 14699, source loss =  0.932, discriminator acc = 0.521\n",
      "Iteration 14799, source loss =  0.876, discriminator acc = 0.676\n",
      "Iteration 14899, source loss =  0.946, discriminator acc = 0.637\n",
      "Iteration 14999, source loss =  0.961, discriminator acc = 0.567\n",
      "PS seed: 98098, model seed: 98237\n",
      "model_FINAL/CellDART_original/dlpfc/GSE144136_spatialLIBD/40markers/3mix_100000spots/minmax/gen_dlpfc_dlpfc-9356/98237\n",
      "Adversarial training for slides dict_keys(['151507', '151508', '151509', '151510', '151669', '151670', '151671', '151672', '151674', '151676']): \n",
      "Train on 100000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 2s 16us/sample - loss: 1.0669 - mae: 0.0311\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 1s 13us/sample - loss: 0.6706 - mae: 0.0228\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.5781 - mae: 0.0207\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.5306 - mae: 0.0196\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.4942 - mae: 0.0187\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.4594 - mae: 0.0179\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.4246 - mae: 0.0170\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 1s 13us/sample - loss: 0.3926 - mae: 0.0161\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.3632 - mae: 0.0154\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 1s 12us/sample - loss: 0.3350 - mae: 0.0146\n",
      "initial_train_done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3525105092096329\n",
      "0.3525105092096329\n",
      "Iteration 99, source loss =  6.861, discriminator acc = 0.800\n",
      "Iteration 199, source loss =  2.939, discriminator acc = 0.973\n",
      "Iteration 299, source loss =  2.650, discriminator acc = 0.370\n",
      "Iteration 399, source loss =  1.683, discriminator acc = 0.769\n",
      "Iteration 499, source loss =  1.682, discriminator acc = 0.891\n",
      "Iteration 599, source loss =  1.721, discriminator acc = 0.806\n",
      "Iteration 699, source loss =  1.492, discriminator acc = 0.709\n",
      "Iteration 799, source loss =  1.408, discriminator acc = 0.817\n",
      "Iteration 899, source loss =  1.509, discriminator acc = 0.336\n",
      "Iteration 999, source loss =  1.416, discriminator acc = 0.292\n",
      "Iteration 1099, source loss =  1.432, discriminator acc = 0.758\n",
      "Iteration 1199, source loss =  1.381, discriminator acc = 0.970\n",
      "Iteration 1299, source loss =  1.434, discriminator acc = 0.822\n",
      "Iteration 1399, source loss =  1.694, discriminator acc = 0.983\n",
      "Iteration 1499, source loss =  1.549, discriminator acc = 0.994\n",
      "Iteration 1599, source loss =  1.764, discriminator acc = 0.854\n",
      "Iteration 1699, source loss =  1.538, discriminator acc = 0.999\n",
      "Iteration 1799, source loss =  1.650, discriminator acc = 0.957\n",
      "Iteration 1899, source loss =  1.577, discriminator acc = 0.975\n",
      "Iteration 1999, source loss =  1.509, discriminator acc = 0.954\n",
      "Iteration 2099, source loss =  1.424, discriminator acc = 0.989\n",
      "Iteration 2199, source loss =  1.754, discriminator acc = 0.972\n",
      "Iteration 2299, source loss =  1.408, discriminator acc = 0.977\n",
      "Iteration 2399, source loss =  1.665, discriminator acc = 0.927\n",
      "Iteration 2499, source loss =  1.567, discriminator acc = 0.910\n",
      "Iteration 2599, source loss =  1.540, discriminator acc = 0.939\n",
      "Iteration 2699, source loss =  1.377, discriminator acc = 0.963\n",
      "Iteration 2799, source loss =  1.368, discriminator acc = 0.894\n",
      "Iteration 2899, source loss =  1.313, discriminator acc = 0.804\n",
      "Iteration 2999, source loss =  1.316, discriminator acc = 0.849\n",
      "Iteration 3099, source loss =  1.294, discriminator acc = 0.569\n",
      "Iteration 3199, source loss =  1.307, discriminator acc = 0.685\n",
      "Iteration 3299, source loss =  1.286, discriminator acc = 0.846\n",
      "Iteration 3399, source loss =  1.274, discriminator acc = 0.936\n",
      "Iteration 3499, source loss =  1.271, discriminator acc = 0.893\n",
      "Iteration 3599, source loss =  1.525, discriminator acc = 0.708\n",
      "Iteration 3699, source loss =  1.352, discriminator acc = 0.405\n",
      "Iteration 3799, source loss =  1.412, discriminator acc = 0.852\n",
      "Iteration 3899, source loss =  1.323, discriminator acc = 0.946\n",
      "Iteration 3999, source loss =  1.309, discriminator acc = 0.883\n",
      "Iteration 4099, source loss =  1.267, discriminator acc = 0.621\n",
      "Iteration 4199, source loss =  1.242, discriminator acc = 0.844\n",
      "Iteration 4299, source loss =  1.251, discriminator acc = 0.417\n",
      "Iteration 4399, source loss =  1.243, discriminator acc = 0.839\n",
      "Iteration 4499, source loss =  1.235, discriminator acc = 0.752\n",
      "Iteration 4599, source loss =  1.253, discriminator acc = 0.758\n",
      "Iteration 4699, source loss =  1.255, discriminator acc = 0.813\n",
      "Iteration 4799, source loss =  1.250, discriminator acc = 0.818\n",
      "Iteration 4899, source loss =  1.226, discriminator acc = 0.519\n",
      "Iteration 4999, source loss =  1.278, discriminator acc = 0.859\n",
      "Iteration 5099, source loss =  1.251, discriminator acc = 0.720\n",
      "Iteration 5199, source loss =  1.266, discriminator acc = 0.517\n",
      "Iteration 5299, source loss =  1.262, discriminator acc = 0.785\n",
      "Iteration 5399, source loss =  1.252, discriminator acc = 0.799\n",
      "Iteration 5499, source loss =  1.259, discriminator acc = 0.731\n",
      "Iteration 5599, source loss =  1.281, discriminator acc = 0.431\n",
      "Iteration 5699, source loss =  1.279, discriminator acc = 0.372\n",
      "Iteration 5799, source loss =  1.200, discriminator acc = 0.405\n",
      "Iteration 5899, source loss =  1.182, discriminator acc = 0.576\n",
      "Iteration 5999, source loss =  1.181, discriminator acc = 0.794\n",
      "Iteration 6099, source loss =  1.150, discriminator acc = 0.721\n",
      "Iteration 6199, source loss =  1.125, discriminator acc = 0.750\n",
      "Iteration 6299, source loss =  1.155, discriminator acc = 0.442\n",
      "Iteration 6399, source loss =  1.186, discriminator acc = 0.546\n",
      "Iteration 6499, source loss =  1.114, discriminator acc = 0.507\n",
      "Iteration 6599, source loss =  1.136, discriminator acc = 0.683\n",
      "Iteration 6699, source loss =  1.073, discriminator acc = 0.552\n",
      "Iteration 6799, source loss =  1.066, discriminator acc = 0.530\n",
      "Iteration 6899, source loss =  1.068, discriminator acc = 0.544\n",
      "Iteration 6999, source loss =  1.033, discriminator acc = 0.561\n",
      "Iteration 7099, source loss =  1.060, discriminator acc = 0.371\n",
      "Iteration 7199, source loss =  1.060, discriminator acc = 0.381\n",
      "Iteration 7299, source loss =  1.049, discriminator acc = 0.606\n",
      "Iteration 7399, source loss =  1.032, discriminator acc = 0.509\n",
      "Iteration 7499, source loss =  1.004, discriminator acc = 0.707\n",
      "Iteration 7599, source loss =  1.010, discriminator acc = 0.520\n",
      "Iteration 7699, source loss =  0.994, discriminator acc = 0.696\n",
      "Iteration 7799, source loss =  1.043, discriminator acc = 0.753\n",
      "Iteration 7899, source loss =  1.302, discriminator acc = 0.516\n",
      "Iteration 7999, source loss =  1.207, discriminator acc = 0.436\n",
      "Iteration 8099, source loss =  1.035, discriminator acc = 0.842\n",
      "Iteration 8199, source loss =  1.136, discriminator acc = 0.438\n",
      "Iteration 8299, source loss =  1.343, discriminator acc = 0.509\n",
      "Iteration 8399, source loss =  1.541, discriminator acc = 0.444\n",
      "Iteration 8499, source loss =  1.128, discriminator acc = 0.764\n",
      "Iteration 8599, source loss =  1.110, discriminator acc = 0.957\n",
      "Iteration 8699, source loss =  1.137, discriminator acc = 0.443\n",
      "Iteration 8799, source loss =  1.109, discriminator acc = 0.456\n",
      "Iteration 8899, source loss =  1.046, discriminator acc = 0.553\n",
      "Iteration 8999, source loss =  1.016, discriminator acc = 0.794\n",
      "Iteration 9099, source loss =  1.026, discriminator acc = 0.592\n",
      "Iteration 9199, source loss =  1.004, discriminator acc = 0.628\n",
      "Iteration 9299, source loss =  1.000, discriminator acc = 0.584\n",
      "Iteration 9399, source loss =  0.979, discriminator acc = 0.719\n",
      "Iteration 9499, source loss =  0.987, discriminator acc = 0.330\n",
      "Iteration 9599, source loss =  1.017, discriminator acc = 0.696\n",
      "Iteration 9699, source loss =  1.058, discriminator acc = 0.444\n",
      "Iteration 9799, source loss =  0.979, discriminator acc = 0.620\n",
      "Iteration 9899, source loss =  0.964, discriminator acc = 0.568\n",
      "Iteration 9999, source loss =  0.960, discriminator acc = 0.770\n",
      "Iteration 10099, source loss =  0.954, discriminator acc = 0.725\n",
      "Iteration 10199, source loss =  0.985, discriminator acc = 0.535\n",
      "Iteration 10299, source loss =  0.932, discriminator acc = 0.516\n",
      "Iteration 10399, source loss =  0.983, discriminator acc = 0.632\n",
      "Iteration 10499, source loss =  0.953, discriminator acc = 0.303\n",
      "Iteration 10599, source loss =  0.926, discriminator acc = 0.559\n",
      "Iteration 10699, source loss =  0.931, discriminator acc = 0.492\n",
      "Iteration 10799, source loss =  0.912, discriminator acc = 0.604\n",
      "Iteration 10899, source loss =  0.927, discriminator acc = 0.845\n",
      "Iteration 10999, source loss =  1.109, discriminator acc = 0.320\n",
      "Iteration 11099, source loss =  0.976, discriminator acc = 0.816\n",
      "Iteration 11199, source loss =  0.992, discriminator acc = 0.855\n",
      "Iteration 11299, source loss =  1.113, discriminator acc = 0.402\n",
      "Iteration 11399, source loss =  1.012, discriminator acc = 0.621\n",
      "Iteration 11499, source loss =  0.957, discriminator acc = 0.486\n",
      "Iteration 11599, source loss =  0.928, discriminator acc = 0.734\n",
      "Iteration 11699, source loss =  0.949, discriminator acc = 0.595\n",
      "Iteration 11799, source loss =  0.953, discriminator acc = 0.658\n",
      "Iteration 11899, source loss =  0.935, discriminator acc = 0.678\n",
      "Iteration 11999, source loss =  0.949, discriminator acc = 0.424\n",
      "Iteration 12099, source loss =  0.978, discriminator acc = 0.800\n",
      "Iteration 12199, source loss =  1.004, discriminator acc = 0.574\n",
      "Iteration 12299, source loss =  0.971, discriminator acc = 0.659\n",
      "Iteration 12399, source loss =  0.999, discriminator acc = 0.377\n",
      "Iteration 12499, source loss =  1.018, discriminator acc = 0.347\n",
      "Iteration 12599, source loss =  0.981, discriminator acc = 0.698\n",
      "Iteration 12699, source loss =  1.008, discriminator acc = 0.580\n",
      "Iteration 12799, source loss =  0.916, discriminator acc = 0.480\n",
      "Iteration 12899, source loss =  0.917, discriminator acc = 0.775\n",
      "Iteration 12999, source loss =  0.973, discriminator acc = 0.347\n",
      "Iteration 13099, source loss =  0.934, discriminator acc = 0.461\n",
      "Iteration 13199, source loss =  0.952, discriminator acc = 0.891\n",
      "Iteration 13299, source loss =  0.974, discriminator acc = 0.823\n",
      "Iteration 13399, source loss =  0.926, discriminator acc = 0.450\n",
      "Iteration 13499, source loss =  0.964, discriminator acc = 0.705\n",
      "Iteration 13599, source loss =  0.924, discriminator acc = 0.795\n",
      "Iteration 13699, source loss =  0.942, discriminator acc = 0.397\n",
      "Iteration 13799, source loss =  0.888, discriminator acc = 0.824\n",
      "Iteration 13899, source loss =  0.898, discriminator acc = 0.362\n",
      "Iteration 13999, source loss =  0.870, discriminator acc = 0.608\n",
      "Iteration 14099, source loss =  0.867, discriminator acc = 0.674\n",
      "Iteration 14199, source loss =  0.875, discriminator acc = 0.660\n",
      "Iteration 14299, source loss =  0.870, discriminator acc = 0.601\n",
      "Iteration 14399, source loss =  0.849, discriminator acc = 0.884\n",
      "Iteration 14499, source loss =  0.863, discriminator acc = 0.393\n",
      "Iteration 14599, source loss =  0.910, discriminator acc = 0.357\n",
      "Iteration 14699, source loss =  0.944, discriminator acc = 0.903\n",
      "Iteration 14799, source loss =  1.060, discriminator acc = 0.799\n",
      "Iteration 14899, source loss =  1.071, discriminator acc = 0.833\n",
      "Iteration 14999, source loss =  1.014, discriminator acc = 0.735\n"
     ]
    }
   ],
   "source": [
    "def train(ps_seed, model_seed):\n",
    "    print(f\"PS seed: {ps_seed}, model seed: {model_seed}\")\n",
    "\n",
    "    model_folder = data_loading.get_model_rel_path(\n",
    "        MODEL_NAME,\n",
    "        model_params[\"model_version\"],\n",
    "        lib_seed_path=str(model_seed),\n",
    "        **data_params,\n",
    "    )\n",
    "\n",
    "    model_folder = os.path.join(MODEL_DIR, model_folder)\n",
    "    if not os.path.isdir(model_folder):\n",
    "        os.makedirs(model_folder)\n",
    "        print(model_folder)\n",
    "        \n",
    "    selected_dir = data_loading.get_selected_dir(\n",
    "        data_loading.get_dset_dir(\n",
    "            data_params[\"data_dir\"],\n",
    "            dset=data_params.get(\"dset\", \"dlpfc\"),\n",
    "        ),\n",
    "        **data_params,\n",
    "    )\n",
    "    # Load spatial data\n",
    "    mat_sp_d, mat_sp_meta_d, st_sample_id_l = data_loading.load_spatial(\n",
    "        selected_dir,\n",
    "        **data_params,\n",
    "    )\n",
    "\n",
    "    # Load sc data\n",
    "    sc_mix_d, lab_mix_d, sc_sub_dict, sc_sub_dict2 = data_loading.load_sc(\n",
    "        selected_dir,\n",
    "        **data_params,\n",
    "        seed_int=ps_seed,\n",
    "    )\n",
    "\n",
    "    target_d = {}\n",
    "    if \"train\" in mat_sp_d:\n",
    "        # keys of dict are splits\n",
    "        for split in mat_sp_d:\n",
    "            target_d[split] = np.concatenate(list(mat_sp_d[split].values()))\n",
    "    else:\n",
    "        # keys of subdicts are splits\n",
    "        for split in next(iter(mat_sp_d.values())):\n",
    "            target_d[split] = np.concatenate((v[split] for v in mat_sp_d.values()))\n",
    "\n",
    "\n",
    "    advtrain_folder = os.path.join(model_folder, \"advtrain\")\n",
    "    pretrain_folder = os.path.join(model_folder, \"pretrain\")\n",
    "    if not os.path.isdir(advtrain_folder):\n",
    "        os.makedirs(advtrain_folder)\n",
    "    if not os.path.isdir(pretrain_folder):\n",
    "        os.makedirs(pretrain_folder)\n",
    "\n",
    "    if data_params.get(\"samp_split\"):\n",
    "        tqdm.write(f\"Adversarial training for slides {mat_sp_d['train'].keys()}: \")\n",
    "        save_folder = os.path.join(advtrain_folder, \"samp_split\")\n",
    "    else:\n",
    "        tqdm.write(f\"Adversarial training for slides {next(iter(mat_sp_d.keys()))}: \")\n",
    "        save_folder = os.path.join(advtrain_folder, \"one_model\")\n",
    "\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    embs, embs_noda, clssmodel, clssmodel_noda = da_cellfraction.train(\n",
    "        sc_mix_d[\"train\"],\n",
    "        lab_mix_d[\"train\"],\n",
    "        target_d[\"train\"],\n",
    "        alpha=train_params.get(\"alpha\", 0.6),\n",
    "        alpha_lr=train_params.get(\"alpha_lr\", 5),\n",
    "        emb_dim=model_params[\"celldart_kwargs\"].get(\"emb_dim\", 64),\n",
    "        batch_size=train_params.get(\"batch_size\", 512),\n",
    "        n_iterations=train_params.get(\"n_iter\", 3000),\n",
    "        initial_train=train_params.get(\"pretraining\", True),\n",
    "        initial_train_epochs=train_params.get(\"initial_train_epochs\", 10),\n",
    "        batch_size_initial_train=max(train_params.get(\"batch_size\", 512), 512),\n",
    "        seed=model_seed,\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "\n",
    "    if not os.path.isdir(os.path.join(save_folder, \"final_model\")):\n",
    "        os.makedirs(os.path.join(save_folder, \"final_model\"))\n",
    "    if not os.path.isdir(os.path.join(pretrain_folder, \"final_model\")):\n",
    "        os.makedirs(os.path.join(pretrain_folder, \"final_model\"))\n",
    "\n",
    "    clssmodel_noda.save(os.path.join(pretrain_folder, \"final_model\", \"model\"))\n",
    "    clssmodel.save(os.path.join(save_folder, \"final_model\", \"model\"))\n",
    "\n",
    "    embs_noda.save(os.path.join(pretrain_folder, \"final_model\", \"embs\"))\n",
    "    embs.save(os.path.join(save_folder, \"final_model\", \"embs\"))\n",
    "\n",
    "    with open(os.path.join(model_folder, \"config.yml\"), \"w\") as f:\n",
    "        yaml.safe_dump(config, f)\n",
    "\n",
    "\n",
    "for ps_seed, model_seed in zip(PS_SEEDS, MODEL_SEEDS):\n",
    "    train(ps_seed, model_seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(2, 2), dtype=float32, device=/device:GPU:0)\n",
      "Evaluating CellDART_original on with 1 jobs\n",
      "Using library config:\n",
      "None\n",
      "Loading config celldart-final-dlpfc-ht.yml ... \n",
      "data_params:\n",
      "  all_genes: false\n",
      "  data_dir: ../AGrEDA/data\n",
      "  dset: dlpfc\n",
      "  n_markers: 40\n",
      "  n_mix: 3\n",
      "  n_spots: 100000\n",
      "  samp_split: true\n",
      "  sc_id: GSE144136\n",
      "  scaler_name: minmax\n",
      "  st_id: spatialLIBD\n",
      "  st_split: false\n",
      "lib_params:\n",
      "  manual_seed: 1846326316\n",
      "model_params:\n",
      "  celldart_kwargs:\n",
      "    bn_momentum: 0.01\n",
      "    emb_dim: 32\n",
      "  model_version: gen_dlpfc_dlpfc-9356\n",
      "train_params:\n",
      "  alpha: 2.0\n",
      "  alpha_lr: 10\n",
      "  batch_size: 512\n",
      "  initial_train_epochs: 10\n",
      "  lr: 0.0001\n",
      "  n_iter: 15000\n",
      "  pretraining: true\n",
      "  reverse_val: false\n",
      "\n",
      "Saving results to results_FINAL/CellDART_original/dlpfc/GSE144136_spatialLIBD/40markers/3mix_100000spots/minmax/gen_dlpfc_dlpfc-9356/98237 ...\n",
      "Loading Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/CellDART/src/da_utils/evaluator.py:159: UserWarning: Output folder exists. Will overwrite.\n",
      "  self.results_folder = self.temp_folder_holder.set_output_folder(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ST adata: \n",
      "Getting predictions: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting Samples\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   12.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   19.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   26.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   32.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   38.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   44.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   51.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   56.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  1.3min finished\n",
      "Calculating domain shift for 151507: TRAIN |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151508: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151509: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151510: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151669: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151670: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151671: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151672: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151674: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151676: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151673: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n",
      "Calculating domain shift for 151675: TRAIN | milisi rf50 | VAL | milisi rf50 | TEST | milisi rf50 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/home/wma/miniconda3/envs/CellDART/lib/python3.8/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Pseudospots (Cosine Distance)  \\\n",
      "                                                                  train   \n",
      "                       SC Split Sample ID                                 \n",
      "Before DA              train    151507                         0.094176   \n",
      "                                151508                         0.094176   \n",
      "                                151509                         0.094176   \n",
      "                                151510                         0.094176   \n",
      "                                151669                         0.094176   \n",
      "                                151670                         0.094176   \n",
      "                                151671                         0.094176   \n",
      "                                151672                         0.094176   \n",
      "                                151674                         0.094176   \n",
      "                                151676                         0.094176   \n",
      "                       val      151673                         0.094176   \n",
      "                       test     151675                         0.094176   \n",
      "After DA (final model) train    151507                         0.263041   \n",
      "                                151508                         0.263041   \n",
      "                                151509                         0.263041   \n",
      "                                151510                         0.263041   \n",
      "                                151669                         0.263041   \n",
      "                                151670                         0.263041   \n",
      "                                151671                         0.263041   \n",
      "                                151672                         0.263041   \n",
      "                                151674                         0.263041   \n",
      "                                151676                         0.263041   \n",
      "                       val      151673                         0.263041   \n",
      "                       test     151675                         0.263041   \n",
      "\n",
      "                                                                   RF50  \\\n",
      "                                                val      test     train   \n",
      "                       SC Split Sample ID                                 \n",
      "Before DA              train    151507     0.218639  0.212868  0.995367   \n",
      "                                151508     0.218639  0.212868  0.993890   \n",
      "                                151509     0.218639  0.212868  0.993487   \n",
      "                                151510     0.218639  0.212868  0.992664   \n",
      "                                151669     0.218639  0.212868  0.997275   \n",
      "                                151670     0.218639  0.212868  0.994093   \n",
      "                                151671     0.218639  0.212868  0.996158   \n",
      "                                151672     0.218639  0.212868  0.994634   \n",
      "                                151674     0.218639  0.212868  0.997950   \n",
      "                                151676     0.218639  0.212868  0.993960   \n",
      "                       val      151673     0.218639  0.212868  0.995826   \n",
      "                       test     151675     0.218639  0.212868  0.995779   \n",
      "After DA (final model) train    151507     0.270366  0.270210  0.999158   \n",
      "                                151508     0.270366  0.270210  0.996874   \n",
      "                                151509     0.270366  0.270210  0.998731   \n",
      "                                151510     0.270366  0.270210  0.998671   \n",
      "                                151669     0.270366  0.270210  0.997876   \n",
      "                                151670     0.270366  0.270210  0.997757   \n",
      "                                151671     0.270366  0.270210  0.999242   \n",
      "                                151672     0.270366  0.270210  0.999850   \n",
      "                                151674     0.270366  0.270210  0.999925   \n",
      "                                151676     0.270366  0.270210  0.999850   \n",
      "                       val      151673     0.270366  0.270210  0.999113   \n",
      "                       test     151675     0.270366  0.270210  0.999029   \n",
      "\n",
      "                                                               \\\n",
      "                                                val      test   \n",
      "                       SC Split Sample ID                       \n",
      "Before DA              train    151507     0.992750  0.996025   \n",
      "                                151508     0.994790  0.996500   \n",
      "                                151509     0.992247  0.993168   \n",
      "                                151510     0.992864  0.995921   \n",
      "                                151669     0.991619  0.992136   \n",
      "                                151670     0.993129  0.992529   \n",
      "                                151671     0.993142  0.994459   \n",
      "                                151672     0.994632  0.995532   \n",
      "                                151674     0.997120  0.997159   \n",
      "                                151676     0.993510  0.995510   \n",
      "                       val      151673     0.993866  0.994253   \n",
      "                       test     151675     0.994805  0.994505   \n",
      "After DA (final model) train    151507     0.998317  0.997825   \n",
      "                                151508     0.999500  0.997990   \n",
      "                                151509     0.998556  0.999700   \n",
      "                                151510     0.997882  0.999600   \n",
      "                                151669     1.000000  0.997551   \n",
      "                                151670     0.994600  0.996743   \n",
      "                                151671     0.998175  0.999292   \n",
      "                                151672     0.999277  0.999177   \n",
      "                                151674     0.999900  0.999020   \n",
      "                                151676     0.998677  0.997855   \n",
      "                       val      151673     0.998426  0.998126   \n",
      "                       test     151675     0.997311  0.997407   \n",
      "\n",
      "                                          miLISI (perplexity=30)            \\\n",
      "                                                           train       val   \n",
      "                       SC Split Sample ID                                    \n",
      "Before DA              train    151507                  1.000000  1.000000   \n",
      "                                151508                  1.000000  1.000000   \n",
      "                                151509                  1.000000  1.000000   \n",
      "                                151510                  1.000000  1.000000   \n",
      "                                151669                  1.000000  1.000000   \n",
      "                                151670                  1.000000  1.000000   \n",
      "                                151671                  1.000000  1.000000   \n",
      "                                151672                  1.000000  1.000000   \n",
      "                                151674                  1.000000  1.000000   \n",
      "                                151676                  1.000000  1.000000   \n",
      "                       val      151673                  1.000000  1.000000   \n",
      "                       test     151675                  1.000000  1.000000   \n",
      "After DA (final model) train    151507                  1.001639  1.001703   \n",
      "                                151508                  1.000000  1.000524   \n",
      "                                151509                  1.000000  1.000000   \n",
      "                                151510                  1.000000  1.000000   \n",
      "                                151669                  1.000000  1.000000   \n",
      "                                151670                  1.000000  1.000000   \n",
      "                                151671                  1.000000  1.000000   \n",
      "                                151672                  1.000000  1.000000   \n",
      "                                151674                  1.001060  1.000000   \n",
      "                                151676                  1.003067  1.001782   \n",
      "                       val      151673                  1.002067  1.001765   \n",
      "                       test     151675                  1.004319  1.004937   \n",
      "\n",
      "                                                     \\\n",
      "                                               test   \n",
      "                       SC Split Sample ID             \n",
      "Before DA              train    151507     1.000000   \n",
      "                                151508     1.000000   \n",
      "                                151509     1.000000   \n",
      "                                151510     1.000000   \n",
      "                                151669     1.000000   \n",
      "                                151670     1.000000   \n",
      "                                151671     1.000000   \n",
      "                                151672     1.000000   \n",
      "                                151674     1.000000   \n",
      "                                151676     1.000000   \n",
      "                       val      151673     1.000000   \n",
      "                       test     151675     1.000000   \n",
      "After DA (final model) train    151507     1.001287   \n",
      "                                151508     1.000000   \n",
      "                                151509     1.000000   \n",
      "                                151510     1.000000   \n",
      "                                151669     1.000000   \n",
      "                                151670     1.000000   \n",
      "                                151671     1.000000   \n",
      "                                151672     1.000000   \n",
      "                                151674     1.000507   \n",
      "                                151676     1.002218   \n",
      "                       val      151673     1.002160   \n",
      "                       test     151675     1.004048   \n",
      "\n",
      "                                          Real Spots (Mean AUC Ex1-10)  \n",
      "                                                                     0  \n",
      "                       SC Split Sample ID                               \n",
      "Before DA              train    151507                        0.552873  \n",
      "                                151508                        0.553010  \n",
      "                                151509                        0.560588  \n",
      "                                151510                        0.540321  \n",
      "                                151669                        0.594138  \n",
      "                                151670                        0.589627  \n",
      "                                151671                        0.563361  \n",
      "                                151672                        0.554380  \n",
      "                                151674                        0.588256  \n",
      "                                151676                        0.566381  \n",
      "                       val      151673                        0.584389  \n",
      "                       test     151675                        0.584004  \n",
      "After DA (final model) train    151507                        0.676322  \n",
      "                                151508                        0.624970  \n",
      "                                151509                        0.517172  \n",
      "                                151510                        0.637428  \n",
      "                                151669                        0.716790  \n",
      "                                151670                        0.725620  \n",
      "                                151671                        0.697910  \n",
      "                                151672                        0.678077  \n",
      "                                151674                        0.628613  \n",
      "                                151676                        0.642631  \n",
      "                       val      151673                        0.642372  \n",
      "                       test     151675                        0.641171  \n",
      "Script run time: 0:15:24.355566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wma/CellDART/src/da_utils/output_utils.py:119: UserWarning: Destination exists. Will overwrite.\n",
      "  warnings.warn(\"Destination exists. Will overwrite.\")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from src.da_models.model_utils.utils import get_metric_ctp\n",
    "from src.da_utils.evaluator import Evaluator\n",
    "\n",
    "\n",
    "metric_ctp = get_metric_ctp(\"cos\")\n",
    "\n",
    "def _device_to_str(device):\n",
    "    return device.name.lstrip(\"/physical_device:\")\n",
    "\n",
    "def get_device(cuda_index=None):\n",
    "    if cuda_index is None:\n",
    "        return _device_to_str(tf.config.list_physical_devices(\"GPU\")[0])\n",
    "\n",
    "    cuda_index = int(cuda_index)\n",
    "\n",
    "    if cuda_index < 0:\n",
    "        return _device_to_str(tf.config.list_physical_devices(\"CPU\")[0])\n",
    "\n",
    "    devices=tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "    if len(devices) > cuda_index:\n",
    "        return _device_to_str(devices[cuda_index])\n",
    "    if len(devices) > 0:\n",
    "        warnings.warn(\"GPU ordinal not valid; using default\", category=UserWarning, stacklevel=2)\n",
    "        return _device_to_str(devices[0])\n",
    "\n",
    "    warnings.warn(\"Using CPU\", category=UserWarning, stacklevel=2)\n",
    "    return _device_to_str(tf.config.list_physical_devices(\"CPU\")[0])\n",
    "\n",
    "def main(args):\n",
    "    evaluator = Evaluator(vars(args), metric_ctp)\n",
    "    evaluator.eval_spots()\n",
    "    evaluator.evaluate_embeddings()\n",
    "    evaluator.eval_sc()\n",
    "\n",
    "    evaluator.produce_results()\n",
    "\n",
    "for ps_seed, model_seed in zip(PS_SEEDS, MODEL_SEEDS):\n",
    "\n",
    "    if model_seed != MODEL_SEEDS[-1]:\n",
    "        continue\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluates.\")\n",
    "    parser.add_argument(\"--pretraining\", \"-p\", action=\"store_true\", help=\"force pretraining\")\n",
    "    parser.add_argument(\"--modelname\", \"-n\", type=str, default=\"ADDA\", help=\"model name\")\n",
    "    parser.add_argument(\"--milisi\", \"-m\", action=\"store_false\", help=\"no milisi\")\n",
    "    parser.add_argument(\"--config_fname\", \"-f\", type=str, help=\"Name of the config file to use\")\n",
    "    parser.add_argument(\"--configs_dir\", \"-cdir\", type=str, default=\"configs\", help=\"config dir\")\n",
    "    parser.add_argument(\n",
    "        \"--njobs\", type=int, default=1, help=\"Number of jobs to use for parallel processing.\"\n",
    "    )\n",
    "    parser.add_argument(\"--cuda\", \"-c\", default=None, help=\"GPU index to use\")\n",
    "    parser.add_argument(\"--tmpdir\", \"-d\", default=None, help=\"optional temporary results directory\")\n",
    "    parser.add_argument(\"--test\", \"-t\", action=\"store_true\", help=\"test mode\")\n",
    "    parser.add_argument(\n",
    "        \"--early_stopping\",\n",
    "        \"-e\",\n",
    "        action=\"store_true\",\n",
    "        help=\"evaluate early stopping. Default: False\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reverse_val\",\n",
    "        \"-r\",\n",
    "        action=\"store_true\",\n",
    "        help=\"use best model through reverse validation. Will use provided\"\n",
    "        \"config file to search across models, then use the one loaded. Default: False\",\n",
    "    )\n",
    "    parser.add_argument(\"--model_dir\", default=\"model\", help=\"model directory\")\n",
    "    parser.add_argument(\"--results_dir\", default=\"results\", help=\"results directory\")\n",
    "    parser.add_argument(\n",
    "        \"--seed_override\",\n",
    "        default=None,\n",
    "        help=\"seed to use for torch and numpy; overrides that in config file\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ps_seed\",\n",
    "        default=-1,\n",
    "        help=\"specific pseudospot seed to use; default of -1 corresponds to 623\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args([\n",
    "        f\"--modelname={MODEL_NAME}\",\n",
    "        f\"--config_fname={CONFIG_FNAME}\",\n",
    "        \"--njobs=1\",\n",
    "        \"--test\",\n",
    "        f\"--model_dir={MODEL_DIR}\",\n",
    "        \"--results_dir=results_FINAL\",\n",
    "        f\"--seed_override={model_seed}\",\n",
    "        f\"--ps_seed={ps_seed}\",\n",
    "        f\"--cuda=0\"\n",
    "    ])\n",
    "\n",
    "    script_start_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        level=logging.WARNING,\n",
    "        format=\"%(asctime)s:%(levelname)s:%(name)s:%(message)s\",\n",
    "    )\n",
    "    with tf.device(get_device(args.cuda)):\n",
    "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "\n",
    "        print(c)\n",
    "\n",
    "        main(args)\n",
    "    print(\"Script run time:\", datetime.datetime.now(datetime.timezone.utc) - script_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CellDART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
